{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CatCodeLab/HealthCatalyst/blob/master/NAT_encoder_decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztdnV_lsH345"
      },
      "source": [
        "# An Encoder–Decoder Network for Neural Machine Translation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "A7tQ1KM7IHyl"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import torch\n",
        "from torchvision.datasets.utils import download_and_extract_archive\n",
        "\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -Uqq ipdb\n",
        "# import ipdb\n",
        "\n",
        "# %pdb on"
      ],
      "metadata": {
        "id": "B96zyUiIraVL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tx-CgTrMc8Sm"
      },
      "outputs": [],
      "source": [
        "# define constants\n",
        "MAX_LENGTH = 20\n",
        "max_words = 50\n",
        "\n",
        "BATCH_SIZE = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooPIV9kGcG44",
        "outputId": "671b53a6-5963-47f6-d907-68ec518d9785"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# check gpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlzssgC5IBeh",
        "outputId": "0917a4fe-1f9e-41df-c558-b9182e6c4762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip to ./spa-eng.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2638744/2638744 [00:00<00:00, 232563448.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./spa-eng.zip to ./\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# download data\n",
        "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
        "\n",
        "filename = 'spa-eng'\n",
        "to_path = './'\n",
        "download_and_extract_archive(url, to_path)\n",
        "text = Path(\"./spa-eng/spa.txt\").read_text()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "8BmMOXOEOzUA"
      },
      "outputs": [],
      "source": [
        "# prepare the data\n",
        "text = text.replace(\"¡\", \"\").replace(\"¿\", \"\")\n",
        "pairs = [line.split(\"\\t\") for line in text.splitlines()]\n",
        "np.random.seed(42)  # extra code – ensures reproducibility on CPU\n",
        "np.random.shuffle(pairs)\n",
        "sentences_en, sentences_es = zip(*pairs)  # separates the pairs into 2 lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AXLuzplCPXEd",
        "outputId": "47fa3829-0599-4dcc-abef-9224ac8311d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "How boring! => Qué aburrimiento!\n",
            "I love sports. => Adoro el deporte.\n",
            "Would you like to swap jobs? => Te gustaría que intercambiemos los trabajos?\n"
          ]
        }
      ],
      "source": [
        "# example data\n",
        "for i in range(3):\n",
        "    print(sentences_en[i], \"=>\", sentences_es[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUBOJgs_A7Cf",
        "outputId": "c87aeb16-48b9-4f32-e7af-a50488669bd1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-16 20:17:48.299527: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 20:17:48.299579: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 20:17:48.300972: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 20:17:49.356106: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
            "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
            "Collecting en-core-web-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "2024-01-16 20:18:02.993592: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-16 20:18:02.993663: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-16 20:18:02.995066: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-16 20:18:04.786758: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'es' are deprecated. Please use the\n",
            "full pipeline package name 'es_core_news_sm' instead.\u001b[0m\n",
            "Collecting es-core-news-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.6.0/es_core_news_sm-3.6.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.1.3)\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n"
          ]
        }
      ],
      "source": [
        "# !pip install --upgrade spacy\n",
        "!python -m spacy download en\n",
        "!python -m spacy download es\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DsK3lGi_hrUu"
      },
      "outputs": [],
      "source": [
        "# option 1: load tokenizer\n",
        "# import spacy\n",
        "# es_tokenizer = spacy.load(\"es_core_news_sm\")\n",
        "# en_tokenizer = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# option 2\n",
        "es_tokenizer = get_tokenizer('spacy', language='es_core_news_sm')\n",
        "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3Ran_QrHcHKU"
      },
      "outputs": [],
      "source": [
        "def build_vocabulary(tokenizer, sentences):\n",
        "        for s in sentences:\n",
        "            yield tokenizer(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NQnjbw2bIhJc"
      },
      "outputs": [],
      "source": [
        "en_vocab = build_vocab_from_iterator(build_vocabulary(en_tokenizer, sentences_en), min_freq=1, specials=['<UNK>', '<PAD>', '<SOS>', '<EOS>'])\n",
        "en_vocab.set_default_index(en_vocab[\"<UNK>\"])\n",
        "\n",
        "es_vocab = build_vocab_from_iterator(build_vocabulary(es_tokenizer, sentences_es), min_freq=1, specials=['<UNK>', '<PAD>', '<SOS>', '<EOS>'])\n",
        "es_vocab.set_default_index(es_vocab[\"<UNK>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-YA4CsVfUzp",
        "outputId": "d88961b5-98f0-485c-a742-868bfc7dc01b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{index of(['here', 'is', 'an', 'example'])} -> [69, 12, 79, 1869]\n",
            "{index of(['Qué', 'aburrimiento'])} -> [49, 5500]\n"
          ]
        }
      ],
      "source": [
        "# Test English vocab\n",
        "print(f\"{{index of(['here', 'is', 'an', 'example'])}} -> {en_vocab(['here', 'is', 'an', 'example'])}\")\n",
        "\n",
        "# Test Spanish vocab\n",
        "print(f\"{{index of(['Qué', 'aburrimiento'])}} -> {es_vocab(['Qué', 'aburrimiento'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "k43rG8PHKbF5"
      },
      "outputs": [],
      "source": [
        "def prepare_data(inp_sents, target_sents, inp_vocab, target_vocab, inp_tokenizer, target_tokenizer):\n",
        "  data = []\n",
        "  for raw_inp, raw_target in zip(inp_sents, target_sents):\n",
        "    inp_tensor_ = torch.tensor([inp_vocab[token] for token in inp_tokenizer(raw_inp)], dtype=torch.long)\n",
        "    target_tensor_ = torch.tensor([target_vocab[token] for token in target_tokenizer(raw_target)], dtype=torch.long)\n",
        "    data.append((inp_tensor_, target_tensor_))\n",
        "\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEB0Csx3NG4H",
        "outputId": "494e8c48-07b8-4a38-c913-03659bf1696d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['How boring!', 'Qué aburrimiento!'], ['I love sports.', 'Adoro el deporte.']]\n"
          ]
        }
      ],
      "source": [
        "print(pairs[:2])\n",
        "\n",
        "train_data = prepare_data(sentences_en[:100_000], sentences_es[:100_000], en_vocab, es_vocab, en_tokenizer, es_tokenizer)\n",
        "val_data = prepare_data(sentences_en[100_000:], sentences_es[100_000:], en_vocab, es_vocab, en_tokenizer, es_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5IifKmW1MFS9"
      },
      "outputs": [],
      "source": [
        "PAD_IDX = en_vocab['<PAD>']\n",
        "SOS_IDX = en_vocab['<SOS>']\n",
        "EOS_IDX = en_vocab['<EOS>']\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def generate_batch(data_batch):\n",
        "  en_batch, es_batch = [], []\n",
        "\n",
        "  for (en_item, es_item) in data_batch:\n",
        "    # en_batch.append(torch.cat([torch.tensor([SOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "    # es_batch.append(torch.cat([torch.tensor([SOS_IDX]), es_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "    en_batch.append(torch.cat([en_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "    es_batch.append(torch.cat([es_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "\n",
        "  en_batch = pad_sequence(en_batch, padding_value=PAD_IDX, batch_first=True)\n",
        "  es_batch = pad_sequence(es_batch, padding_value=PAD_IDX, batch_first=True)\n",
        "\n",
        "  return en_batch.to(device), es_batch.to(device)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "valid_loader = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "IUG7zUHWddYd"
      },
      "outputs": [],
      "source": [
        "# # build dataset\n",
        "# class SentencesDataset(Dataset):\n",
        "\n",
        "#   def __init__(self, source, target):\n",
        "#     self.source = source\n",
        "#     self.target = target\n",
        "\n",
        "#   def __len__(self):\n",
        "#     return len(self.target)\n",
        "\n",
        "#   def __getitem__(self, idx):\n",
        "#         source = self.source[idx]\n",
        "#         target = self.target[idx]\n",
        "#         sample = {\"source\": source, \"target\": target}\n",
        "#         return sample\n",
        "\n",
        "# # generate dataset\n",
        "# train_dataset = SentencesDataset(sentences_en[:100_000], sentences_es[:100_000])\n",
        "# test_dataset = SentencesDataset(sentences_en[100_000:], sentences_es[100_000:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "KfJMHndEzaPq"
      },
      "outputs": [],
      "source": [
        "# def vectorize_batch(batch):\n",
        "#     en_sen, es_sen = list(zip(*batch))\n",
        "#     X = [en_vocab(en_tokenizer(text)) for text in en_sen]\n",
        "#     X = [tokens+([0]* (max_words-len(tokens))) if len(tokens)<max_words else tokens[:max_words] for tokens in X] ## Bringing all samples to max_words length.\n",
        "\n",
        "#     Y = [es_vocab(es_tokenizer(text)) for text in es_sen]\n",
        "#     Y = [tokens+([0]* (max_words-len(tokens))) if len(tokens)<max_words else tokens[:max_words] for tokens in X] ## Bringing all samples to max_words length.\n",
        "\n",
        "#     return torch.tensor(X, dtype=torch.int32), torch.tensor(Y) - 1 ## We have deducted 1 from target names to get them in range [0,1,2,3] from [1,2,3,4]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "q6TMpHuaxgQu"
      },
      "outputs": [],
      "source": [
        "# train_loader = DataLoader(train_dataset, batch_size=1024, collate_fn=vectorize_batch, shuffle=True)\n",
        "# test_loader  = DataLoader(test_dataset , batch_size=1024, collate_fn=vectorize_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "imp1fqlGf6UR"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        # print(f'In encoder: embedding size [{input_size}, {hidden_size}]')\n",
        "\n",
        "    def forward(self, input):\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, hidden = self.gru(embedded)\n",
        "        # print(f'In encoder: output size [{output.size()}, hidden_size = {hidden.size()}]')\n",
        "        return output, hidden\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "_MDCANPr59TS"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        # print(f'In decoder: embedding size [{output_size}, {hidden_size}]')\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "\n",
        "        # print(f'decoder forward: batch_size {batch_size}, encoder_outputs size = {encoder_outputs.size()}')\n",
        "        # decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(2)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "\n",
        "        # print(f'decoder forward: decoder_input {decoder_input.size()}')\n",
        "\n",
        "        # for i in range(MAX_LENGTH):\n",
        "        max_words_length = MAX_LENGTH\n",
        "        if target_tensor is not None:\n",
        "          max_words_length = target_tensor.size(1)\n",
        "\n",
        "        # for i in range(target_tensor.size(1)):\n",
        "        for i in range(max_words_length):\n",
        "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
        "            decoder_outputs.append(decoder_output)\n",
        "            # print(decoder_output.size())\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                # print(f'we have target_tensor size = {target_tensor.size()}')\n",
        "                # [128, 26])\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "                # print(f'we have decoder_input size = {decoder_input.size()}')\n",
        "                # torch.Size([128, 1])\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                # print(f'decoder_output size = {decoder_output.size()}')\n",
        "                # torch.Size([1, 1, 29103]\n",
        "                # _, topi = decoder_output.topk(1, dim=2)\n",
        "                # If dim is not given, the last dimension of the input is chosen for topk.\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                # print(f'topi.size={topi.size()}')\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "                # print(f'In decoder, decoder_input.size={decoder_input.size()}')\n",
        "\n",
        "        # ipdb.set_trace()\n",
        "        # print(f'decoder_outputs.size={len(decoder_outputs)}')\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        # print(f'Before log_softmax: decoder_outputs.size={decoder_outputs.size()}')\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        # print(f'After log_softmax: decoder_outputs.size={decoder_outputs.size()}')\n",
        "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
        "\n",
        "    def forward_step(self, input, hidden):\n",
        "        # print(f'forward_step: input_size = {input.size()}')\n",
        "        output = self.embedding(input)\n",
        "        # print(f'forward_step: output_size = {output.size()}')\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.out(output)\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Va = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, query, keys):\n",
        "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        context = torch.bmm(weights, keys)\n",
        "\n",
        "        return context, weights\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attention = BahdanauAttention(hidden_size)\n",
        "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(2)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "        attentions = []\n",
        "\n",
        "        # for i in range(MAX_LENGTH):\n",
        "        max_words_length = MAX_LENGTH\n",
        "        if target_tensor is not None:\n",
        "          max_words_length = target_tensor.size(1)\n",
        "\n",
        "        # for i in range(target_tensor.size(1)):\n",
        "        for i in range(max_words_length):\n",
        "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            decoder_outputs.append(decoder_output)\n",
        "            attentions.append(attn_weights)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        attentions = torch.cat(attentions, dim=1)\n",
        "\n",
        "        return decoder_outputs, decoder_hidden, attentions\n",
        "\n",
        "\n",
        "    def forward_step(self, input, hidden, encoder_outputs):\n",
        "        embedded =  self.dropout(self.embedding(input))\n",
        "\n",
        "        query = hidden.permute(1, 0, 2)\n",
        "        context, attn_weights = self.attention(query, encoder_outputs)\n",
        "        input_gru = torch.cat((embedded, context), dim=2)\n",
        "\n",
        "        output, hidden = self.gru(input_gru, hidden)\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output, hidden, attn_weights"
      ],
      "metadata": {
        "id": "VKeya6mD65qW"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "GmnfYG-nPfqI"
      },
      "outputs": [],
      "source": [
        "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
        "          decoder_optimizer, criterion):\n",
        "\n",
        "    total_loss = 0\n",
        "    for data in dataloader:\n",
        "        input_tensor, target_tensor = data\n",
        "\n",
        "        # print(f'input_tensor size = {input_tensor.size()}, target_tensor size = {target_tensor.size()}')\n",
        "\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "\n",
        "        # print(f'encoder_output[{encoder_outputs.size()}], decoder_output[{decoder_outputs.size()}], encoder_output[{target_tensor.size()}] ')\n",
        "        loss = criterion(\n",
        "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "            target_tensor.view(-1)\n",
        "        )\n",
        "        loss.backward()\n",
        "\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "WK-j2fOsSzRi"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "UBBn-SI-TLRg"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "cDK-9Ly-8vsz"
      },
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "\n",
        "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
        "               print_every=100, plot_every=100):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
        "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
        "\n",
        "        if epoch % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tensorFromSentence(inp_vocab, inp_tokenizer, sentence):\n",
        "\n",
        "    inp_tensor_ = torch.tensor([inp_vocab[token] for token in inp_tokenizer(sentence)], dtype=torch.long)\n",
        "    # inp_tensor_  = torch.cat([inp_tensor_, torch.tensor([EOS_IDX])], dim=0)\n",
        "\n",
        "    inp_batch = []\n",
        "    inp_batch.append(torch.cat([inp_tensor_, torch.tensor([EOS_IDX])], dim=0))\n",
        "\n",
        "    inp_batch = pad_sequence(inp_batch, padding_value=PAD_IDX, batch_first=True).to(device)\n",
        "    print(f'inp_batch size = {inp_batch.size()}')\n",
        "    return inp_batch\n",
        "\n",
        "    # return torch.tensor(concat_tensor.clone().detach(), dtype=torch.long, device=device).view(1, -1)"
      ],
      "metadata": {
        "id": "_-ugmwa_LfIn"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(encoder, decoder, sentence, inp_vocab, inp_tokenizer):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(inp_vocab, inp_tokenizer, sentence)\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        print(f'encoder_outputs.size = {encoder_outputs.size()}, encoder_hidden.size={encoder_hidden.size()}')\n",
        "        decoder_outputs, decoder_hidden, _ = decoder(encoder_outputs, encoder_hidden)\n",
        "\n",
        "        print(f'after decoder: {decoder_outputs.size()}')\n",
        "        _, topi = decoder_outputs.topk(1, dim=2)\n",
        "        decoded_ids = topi.squeeze().tolist()\n",
        "        print(f'index: {decoded_ids}')\n",
        "\n",
        "        decoded_words = []\n",
        "        decoded_words = es_vocab.lookup_tokens(decoded_ids)\n",
        "        print(f'words: {decoded_words}')\n",
        "        # for idx in decoded_ids:\n",
        "        #     if idx.item() == 2:\n",
        "        #         decoded_words.append('<EOS>')\n",
        "        #         break\n",
        "        #     print(f'{idx}: {decoded_words}, {idx}')\n",
        "        #     # decoded_words.append(es_vocab.lookup_tokens(idx.item()))\n",
        "\n",
        "    return decoded_words"
      ],
      "metadata": {
        "id": "DedUmuaXKcVC"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uGgF-IvOYrCc"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3pQc2d8TZBQ"
      },
      "source": [
        "# Training and Evaluating\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sKqTBLEzTffa",
        "outputId": "332deaff-ece5-4495-d66f-c2e09ccc2353"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English vocab size: 14831\n",
            "Spanish vocab size: 29103\n",
            "0m 50s (- 41m 29s) (1 2%) 2.2690\n",
            "1m 40s (- 40m 23s) (2 4%) 1.5799\n",
            "2m 31s (- 39m 31s) (3 6%) 1.2858\n",
            "3m 21s (- 38m 39s) (4 8%) 1.0907\n",
            "4m 12s (- 37m 52s) (5 10%) 0.9469\n",
            "5m 3s (- 37m 2s) (6 12%) 0.8395\n",
            "5m 53s (- 36m 9s) (7 14%) 0.7630\n",
            "6m 44s (- 35m 21s) (8 16%) 0.6888\n",
            "7m 34s (- 34m 31s) (9 18%) 0.6362\n",
            "8m 25s (- 33m 40s) (10 20%) 0.5912\n",
            "9m 15s (- 32m 49s) (11 22%) 0.5553\n",
            "10m 6s (- 31m 59s) (12 24%) 0.5189\n",
            "10m 56s (- 31m 8s) (13 26%) 0.4894\n",
            "11m 46s (- 30m 17s) (14 28%) 0.4657\n",
            "12m 37s (- 29m 27s) (15 30%) 0.4455\n",
            "13m 28s (- 28m 37s) (16 32%) 0.4237\n",
            "14m 18s (- 27m 47s) (17 34%) 0.4077\n",
            "15m 9s (- 26m 56s) (18 36%) 0.3941\n",
            "16m 0s (- 26m 6s) (19 38%) 0.3766\n",
            "16m 50s (- 25m 16s) (20 40%) 0.3653\n",
            "17m 41s (- 24m 25s) (21 42%) 0.3542\n",
            "18m 31s (- 23m 34s) (22 44%) 0.3427\n",
            "19m 23s (- 22m 46s) (23 46%) 0.3348\n",
            "20m 14s (- 21m 55s) (24 48%) 0.3241\n",
            "21m 4s (- 21m 4s) (25 50%) 0.3169\n",
            "21m 55s (- 20m 14s) (26 52%) 0.3071\n",
            "22m 45s (- 19m 23s) (27 54%) 0.3015\n",
            "23m 36s (- 18m 33s) (28 56%) 0.2942\n",
            "24m 27s (- 17m 42s) (29 57%) 0.2860\n",
            "25m 17s (- 16m 51s) (30 60%) 0.2816\n",
            "26m 7s (- 16m 0s) (31 62%) 0.2752\n",
            "26m 58s (- 15m 10s) (32 64%) 0.2701\n",
            "27m 48s (- 14m 19s) (33 66%) 0.2640\n",
            "28m 39s (- 13m 29s) (34 68%) 0.2591\n",
            "29m 30s (- 12m 38s) (35 70%) 0.2536\n",
            "30m 20s (- 11m 48s) (36 72%) 0.2499\n",
            "31m 11s (- 10m 57s) (37 74%) 0.2452\n",
            "32m 2s (- 10m 7s) (38 76%) 0.2417\n",
            "32m 53s (- 9m 16s) (39 78%) 0.2366\n",
            "33m 44s (- 8m 26s) (40 80%) 0.2336\n",
            "34m 34s (- 7m 35s) (41 82%) 0.2294\n",
            "35m 25s (- 6m 44s) (42 84%) 0.2263\n",
            "36m 15s (- 5m 54s) (43 86%) 0.2222\n",
            "37m 6s (- 5m 3s) (44 88%) 0.2194\n",
            "37m 56s (- 4m 12s) (45 90%) 0.2171\n",
            "38m 47s (- 3m 22s) (46 92%) 0.2140\n",
            "39m 37s (- 2m 31s) (47 94%) 0.2106\n",
            "40m 28s (- 1m 41s) (48 96%) 0.2086\n",
            "41m 18s (- 0m 50s) (49 98%) 0.2035\n",
            "42m 9s (- 0m 0s) (50 100%) 0.2033\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "hidden_size = 128\n",
        "\n",
        "print(f'English vocab size: {len(en_vocab)}')\n",
        "print(f'Spanish vocab size: {len(es_vocab)}')\n",
        "\n",
        "encoder = EncoderRNN(len(en_vocab), hidden_size).to(device)\n",
        "# decoder = DecoderRNN(hidden_size, len(es_vocab)).to(device)\n",
        "decoder = AttnDecoderRNN(hidden_size, len(es_vocab)).to(device)\n",
        "\n",
        "train(train_loader, encoder, decoder, 50, print_every=1, plot_every=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def evaluateRandomly(encoder, decoder, n=1):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words = evaluate(encoder, decoder, pair[0], en_vocab, en_tokenizer)\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "metadata": {
        "id": "IGoQ2lwn_bxQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder.eval()\n",
        "decoder.eval()\n"
      ],
      "metadata": {
        "id": "KRRJz7wQYqED",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "90b92451-a4bd-431d-d122-9cf97dd7008d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'encoder' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-88531b0948db>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'encoder' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateRandomly(encoder, decoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "id": "3w-K1Ml-MmEH",
        "outputId": "0d9a451b-dd1e-4efb-89bd-6b29ff161766"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'encoder' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c30f6ff1c07f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluateRandomly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'encoder' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPVOp5RbQDInxgTgMolcKix",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}